name: Crawl travel packages (nightly)

on:
  schedule:
    - cron: "15 14 * * *"   # runs daily 14:15 UTC (03:15 NZDT)
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

concurrency:
  group: crawl-packages
  cancel-in-progress: false

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout DEV branch
        uses: actions/checkout@v4
        with:
          ref: dev           # ðŸ‘ˆ checkout dev, not main
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          pip install -r scraper/requirements.txt
          # If using Playwright in spiders:
          python -m playwright install --with-deps chromium

      - name: Run crawl
        working-directory: scraper
        env:
          # add any secrets your spiders need (API keys etc.)
          PYTHONUNBUFFERED: "1"
        run: |
          mkdir -p out
          # Crawl each source (tweak CLOSESPIDER_PAGECOUNT or remove for full run)
          scrapy crawl flightcentre_sitemap -O out/fc.jsonl || true
          scrapy crawl flightcentre         -O out/fc_pw.jsonl -s CLOSESPIDER_PAGECOUNT=80 || true
          scrapy crawl helloworld           -O out/hw.jsonl || true
          scrapy crawl helloworld_cruise    -O out/hwcr.jsonl || true
          scrapy crawl houseoftravel        -O out/hot.jsonl || true
          scrapy crawl worldtravellers      -O out/wt.jsonl || true

          # Combine, fixups, validate (adjust to your repoâ€™s scripts)
          cat out/*.jsonl > out/packages.all.jsonl
          python scripts/fx_apply.py out/packages.all.jsonl out/packages.final.jsonl || cp out/packages.all.jsonl out/packages.final.jsonl
          python scripts/validate.py out/packages.final.jsonl out/packages.validated.jsonl || cp out/packages.final.jsonl out/packages.validated.jsonl

      - name: Publish dataset to repo (DEV)
        run: |
          mkdir -p public/data
          cp scraper/out/packages.validated.jsonl public/data/packages.final.jsonl

          # Commit only if there are changes
          if ! git diff --quiet -- public/data/packages.final.jsonl; then
            git config user.name  "github-actions[bot]"
            git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
            git add public/data/packages.final.jsonl
            git commit -m "chore(data): update packages dataset"
            # ðŸ‘‡ push to DEV, not main
            git push origin HEAD:dev
          else
            echo "No dataset changes."
          fi

      # Optional: automatically open/refresh a PR from dev â†’ main so you can review & merge
      - name: Open PR to main
        if: always()
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "chore(data): update packages dataset"
          branch: dev
          base: main
          title: "chore(data): nightly dataset update"
          body: "Automated dataset update from nightly crawl."
          labels: automated, data
          draft: true
