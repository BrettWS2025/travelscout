name: Crawl Travel Packages Nightly

on:
  schedule:
    - cron: "25 12 * * *"
  workflow_dispatch:
    inputs:
      spiders:
        description: "Space-separated spiders to run (leave blank for defaults)"
        required: false
        default: ""

permissions:
  contents: write

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout dev branch
        uses: actions/checkout@v4
        with:
          ref: Dev

      - name: Checkout MAIN branch
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r scraper/requirements.txt

      - name: Install Playwright browser
        run: |
          python -m playwright install --with-deps chromium

      - name: Crawl sources
        working-directory: scraper
        env:
          INPUT_SPIDERS: ${{ github.event.inputs.spiders }}
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p out

          # Sitemap-first, then JS/listings (FC sitemap first)
          DEFAULT="helloworld helloworld_cruise houseoftravel_sitemap houseoftravel worldtravellers_sitemap worldtravellers flightcentre_sitemap flightcentre"
          SPIDERS="${INPUT_SPIDERS:-$DEFAULT}"

          echo "Running spiders: $SPIDERS"
          for s in $SPIDERS; do
            echo "::group::Crawling $s"
            scrapy crawl "$s" -O "out/${s}.raw.jsonl" || true
            echo "::endgroup::"
          done

      - name: Normalize & dedupe
        working-directory: scraper
        shell: bash
        run: |
          set -euo pipefail
          shopt -s nullglob

          for raw in out/*.raw.jsonl; do
            name=$(basename "$raw" .raw.jsonl)
            echo "::group::Normalize $name"
            python scripts/normalize.py "$raw" "out/${name}.norm.jsonl" || true
            echo "::endgroup::"
          done

          for norm in out/*.norm.jsonl; do
            name=$(basename "$norm" .norm.jsonl)
            echo "::group::Dedupe $name"
            python scripts/dedupe.py "$norm" "out/${name}.dedupe.jsonl" || true
            echo "::endgroup::"
          done

          # Merge all deduped rows for downstream steps
          dedupes=(out/*.dedupe.jsonl)
          if ((${#dedupes[@]})); then
            cat "${dedupes[@]}" > out/packages.all.jsonl
          else
            : > out/packages.all.jsonl
          fi

      - name: Ensure 'price' field exists
        working-directory: scraper
        shell: bash
        run: |
          set -euo pipefail
          if [[ -s out/packages.all.jsonl ]]; then
            python scripts/ensure_price_field.py out/packages.all.jsonl out/packages.all.coerced.jsonl
          else
            : > out/packages.all.coerced.jsonl
          fi

      - name: Backfill prices for Flight Centre
        working-directory: scraper
        shell: bash
        run: |
          set -euo pipefail
          if [[ -s out/packages.all.coerced.jsonl ]]; then
            # If backfill script fails for some reason, fall back to the coerced file
            python scripts/backfill_prices_flightcentre.py out/packages.all.coerced.jsonl out/packages.all.pricefix.jsonl || \
              cp out/packages.all.coerced.jsonl out/packages.all.pricefix.jsonl
          else
            : > out/packages.all.pricefix.jsonl
          fi

      - name: FX & derived fields
        working-directory: scraper
        shell: bash
        run: |
          set -euo pipefail
          if [[ -s out/packages.all.pricefix.jsonl ]]; then
            python scripts/fx_apply.py out/packages.all.pricefix.jsonl out/packages.final.jsonl
          else
            : > out/packages.final.jsonl
          fi

      - name: Commit dataset into public/data
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p public/data
          cp scraper/out/packages.final.jsonl public/data/packages.final.jsonl

          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          git add public/data/packages.final.jsonl
          git commit -m "chore(data): update packages dataset" || echo "No changes to commit"

          # Push to both branches
          git push origin HEAD:Dev
          git push origin HEAD:main
