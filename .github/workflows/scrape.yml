name: Crawl Travel Packages Nightly

on:
  workflow_dispatch:
    inputs:
      spiders:
        description: "Space-separated spiders to run (leave blank for defaults)"
        required: false
        default: ""

permissions:
  contents: write

# Prevent overlapping pushes from concurrent runs
concurrency:
  group: crawl-travel-packages
  cancel-in-progress: false

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout main (full history)
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0
          persist-credentials: true
          clean: true  # ensure a clean workspace at start

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install -r scraper/requirements.txt

      - name: Install Playwright browser
        run: |
          set -euo pipefail
          python -m playwright install --with-deps chromium

      - name: Crawl sources
        working-directory: scraper
        env:
          INPUT_SPIDERS: ${{ github.event.inputs.spiders }}
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p out

          # Sitemap-first, then JS/listings (FC sitemap first)
          DEFAULT="helloworld helloworld_cruise houseoftravel_sitemap houseoftravel worldtravellers_sitemap worldtravellers flightcentre_sitemap flightcentre"
          SPIDERS="${INPUT_SPIDERS:-$DEFAULT}"

          echo "Running spiders: $SPIDERS"
          for s in $SPIDERS; do
            echo "::group::Crawling $s"
            scrapy crawl "$s" -O "out/${s}.raw.jsonl" || true
            echo "::endgroup::"
          done

      - name: Normalize & dedupe
        working-directory: scraper
        shell: bash
        run: |
          set -euo pipefail
          shopt -s nullglob

          for raw in out/*.raw.jsonl; do
            name=$(basename "$raw" .raw.jsonl)
            echo "::group::Normalize $name"
            python scripts/normalize.py "$raw" "out/${name}.norm.jsonl" || true
            echo "::endgroup::"
          done

          for norm in out/*.norm.jsonl; do
            name=$(basename "$norm" .norm.jsonl)
            echo "::group::Dedupe $name"
            python scripts/dedupe.py "$norm" "out/${name}.dedupe.jsonl" || true
            echo "::endgroup::"
          done

          # Merge all deduped rows for downstream steps
          dedupes=(out/*.dedupe.jsonl)
          if ((${#dedupes[@]})); then
            cat "${dedupes[@]}" > out/packages.all.jsonl
          else
            : > out/packages.all.jsonl
          fi

      - name: Ensure 'price' field exists
        working-directory: scraper
        shell: bash
        run: |
          set -euo pipefail
          if [[ -s out/packages.all.jsonl ]]; then
            python scripts/ensure_price_field.py out/packages.all.jsonl out/packages.all.coerced.jsonl
          else
            : > out/packages.all.coerced.jsonl
          fi

      - name: Backfill prices for Flight Centre
        working-directory: scraper
        shell: bash
        run: |
          set -euo pipefail
          if [[ -s out/packages.all.coerced.jsonl ]]; then
            python scripts/backfill_prices_flightcentre.py out/packages.all.coerced.jsonl out/packages.all.pricefix.jsonl || \
              cp out/packages.all.coerced.jsonl out/packages.all.pricefix.jsonl
          else
            : > out/packages.all.pricefix.jsonl
          fi

      - name: FX & derived fields
        working-directory: scraper
        shell: bash
        run: |
          set -euo pipefail
          if [[ -s out/packages.all.pricefix.jsonl ]]; then
            python scripts/fx_apply.py out/packages.all.pricefix.jsonl out/packages.final.jsonl
          else
            : > out/packages.final.jsonl
          fi

      # IMPORTANT: do NOT write into the repo yet; keep the dataset only in /tmp
      - name: Stage dataset to temp (keep repo clean)
        shell: bash
        run: |
          set -euo pipefail
          cp scraper/out/packages.final.jsonl /tmp/packages.final.jsonl

      - name: Configure Git
        shell: bash
        run: |
          set -euo pipefail
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --global --add safe.directory "$GITHUB_WORKSPACE"

      - name: Commit & push dataset to main and Dev (rebase-safe)
        shell: bash
        run: |
          set -euo pipefail
          DATA_TMP="/tmp/packages.final.jsonl"
          TARGET_PATH="public/data/packages.final.jsonl"

          # Make sure the remotes are up to date
          git fetch origin main Dev || true

          for BR in main Dev; do
            echo "::group::Update $BR"

            # Create or switch to the local branch tracking origin/$BR
            if git show-ref --verify --quiet "refs/heads/$BR"; then
              git checkout "$BR"
            else
              git checkout -b "$BR" "origin/$BR"
            fi

            # Ensure a clean tree before rebase
            git reset --hard
            git pull --rebase origin "$BR"

            # Copy dataset and commit if changed
            mkdir -p public/data
            cp -f "$DATA_TMP" "$TARGET_PATH"
            git add "$TARGET_PATH"

            if git diff --cached --quiet; then
              echo "No changes to commit on $BR"
            else
              git commit -m "chore(data): update packages dataset"
              git push origin "$BR"
            fi
            echo "::endgroup::"
          done
