name: Crawl Travel Packages Nightly

on:
  schedule:
    - cron: "25 12 * * *"   # 12:25 UTC (~01:25 NZT during DST)
  workflow_dispatch:
    inputs:
      spiders:
        description: "Space-separated spiders to run (leave blank for defaults)"
        required: false
        default: ""

permissions:
  contents: write

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout MAIN branch
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          pip install -r scraper/requirements.txt

      - name: Install Playwright browser
        run: |
          python -m playwright install --with-deps chromium

      - name: Crawl sources
        working-directory: scraper
        env:
          INPUT_SPIDERS: ${{ github.event.inputs.spiders }}
        run: |
          set -e
          mkdir -p out

          # Default set for scheduled runs (includes helloworld_cruise)
          DEFAULT="worldtravellers worldtravellers_sitemap houseoftravel houseoftravel_sitemap helloworld helloworld_cruise flightcentre_sitemap flightcentre"
          SPIDERS="${INPUT_SPIDERS:-$DEFAULT}"

          echo "Running spiders: $SPIDERS"
          for s in $SPIDERS; do
            echo "::group::Crawling $s"
            scrapy crawl "$s" -O "out/${s}.raw.jsonl" || true
            echo "::endgroup::"
          done

      - name: "Normalize & dedupe"   # quoted because of '&'
        working-directory: scraper
        run: |
          set -e

          # Normalize every raw file we produced
          shopt -s nullglob
          for raw in out/*.raw.jsonl; do
            name=$(basename "$raw" .raw.jsonl)
            echo "::group::Normalize $name"
            python scripts/normalize.py "$raw" "out/${name}.norm.jsonl" || true
            echo "::endgroup::"
          done

          # Dedupe every normalized file we produced
          for norm in out/*.norm.jsonl; do
            name=$(basename "$norm" .norm.jsonl)
            echo "::group::Dedupe $name"
            python scripts/dedupe.py "$norm" "out/${name}.dedupe.jsonl" || true
            echo "::endgroup::"
          done

          # Merge all deduped outputs (whatever ran)
          cat out/*.dedupe.jsonl > out/packages.all.jsonl || true

          # Apply FX/derived fields if there is any content
          if [ -s out/packages.all.jsonl ]; then
            python scripts/fx_apply.py out/packages.all.jsonl out/packages.final.jsonl
          else
            echo "{}" > out/packages.final.jsonl
          fi

      - name: Commit dataset into public/data
        run: |
          mkdir -p public/data
          cp scraper/out/packages.final.jsonl public/data/packages.final.jsonl

          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          git add public/data/packages.final.jsonl
          git commit -m "chore(data): update packages dataset" || echo "No changes to commit"

          # Explicit push to main
          git push origin HEAD:main
