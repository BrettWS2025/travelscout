name: Analyze deals with OpenAI

on:
  workflow_dispatch: {}
  # To run automatically each day, uncomment one of the schedules below.
  # NOTE: GitHub schedules use UTC and do not adjust for NZ daylight saving.
  # - 06:00 NZDT ≈ 17:00 UTC
  # - 06:00 NZST ≈ 18:00 UTC
  # schedule:
  #   - cron: "0 17 * * *"  # ~06:00 NZDT
  #   - cron: "0 18 * * *"  # ~06:00 NZST

permissions:
  contents: write

concurrency:
  group: analyze-deals
  cancel-in-progress: false

jobs:
  analyze:
    runs-on: ubuntu-latest

    steps:
      - name: Check out repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: main

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"  # speeds up dependency installs

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f "scraper/requirements.txt" ]; then
            pip install -r scraper/requirements.txt
          else
            pip install -r requirements.txt
          fi

      - name: Restore page cache (deal pages HTML)
        uses: actions/cache@v4
        with:
          path: scraper/.cache/pages
          key: pages-${{ hashFiles('public/data/packages.final*.jsonl') }}
          restore-keys: |
            pages-

      # Optional: quick visibility into your dataset on the runner
      # - name: Debug list data dir
      #   run: |
      #     ls -la public/data || true
      #     wc -l public/data/packages.final*.jsonl || true

      - name: Run analysis script
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_MODEL: gpt-4o-mini
          TOP_N: 10              # write per-deal JSON only for Top 20
          PRUNE_PERCENTILE: 70   # keep cheapest 60% by heuristic before OpenAI
          SHORTLIST_SIZE: 400    # cap analysis set after pruning (0=off)
          PARALLEL_FETCH: 12     # concurrent page fetches
          READ_TIMEOUT: 10       # seconds per request
          CACHE_PAGES: 1         # use disk cache for page HTML
          PAUSE_BETWEEN_CALLS: 0.15
          MAX_DEALS: 200           # 0 = analyze all (after nights filter)
          # REPO_ROOT: ${{ github.workspace }}                 # optional override
          # PACKAGES_FILE: public/data/packages.final.jsonl     # optional override
        run: |
          python scraper/scripts/analyze_deals.py

      - name: Commit and push generated report (if any changes)
        run: |
          set -e

          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # Make sure we have the latest remote history
          git fetch origin main

          # Stage everything created by the script
          git add -A

          # If nothing is staged, exit quietly
          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          # Commit first (so the index is clean before rebase)
          git commit -m "chore(reports): OpenAI Top deals $(date -u +'%Y-%m-%dT%H:%M:%SZ')"

          # Rebase our new commit(s) onto the latest remote main
          git pull --rebase --autostash origin main

          # Push (fast-forward expected after rebase)
          git push origin HEAD:main
