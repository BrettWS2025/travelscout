name: Scrape events

on:
  workflow_dispatch:
    inputs:
      spiders:
        description: "Comma-separated spider names to run"
        default: "auckland_events"
        required: true
      feed_path:
        description: "Relative path under scraper/ to write the JSON Lines feed"
        default: "data/Events.jsonl"
        required: true

jobs:
  scrape:
    name: Run Scrapy spiders
    runs-on: ubuntu-latest

    # Run every shell step inside the scraper/ directory
    defaults:
      run:
        shell: bash
        working-directory: scraper

    env:
      # Tell Scrapy which settings module to use
      SCRAPY_SETTINGS_MODULE: tscraper.settings
      # Make sure Python can import tscraper from ./scraper
      PYTHONPATH: ${{ github.workspace }}/scraper
      PYTHONUNBUFFERED: "1"
      PYTHONIOENCODING: "utf-8"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright (Chromium) with system deps
        run: |
          python -m playwright install --with-deps chromium

      - name: Prepare folders
        run: |
          mkdir -p debug httpcache
          mkdir -p "$(dirname "${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}")"

      - name: Sanity check Python package resolution
        run: |
          python - <<'PY'
          import os, sys, pkgutil
          print("CWD:", os.getcwd())
          print("PYTHONPATH:", os.environ.get("PYTHONPATH"))
          import tscraper
          import tscraper.spiders
          print("tscraper path:", list(tscraper.__path__))
          print("discovered spiders:", [m.name for m in pkgutil.iter_modules(tscraper.spiders.__path__)])
          PY

      - name: Show Scrapy version & discovered spiders
        run: |
          scrapy version -v
          scrapy list | tee debug/spiders.txt

      - name: Run spiders
        run: |
          set -euo pipefail
          FEED_PATH="${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}"
          # Use FEEDS at the CLI so it does NOT affect other spiders globally
          FEEDS_JSON="{\"$FEED_PATH\": {\"format\": \"jsonlines\", \"encoding\": \"utf8\", \"overwrite\": false}}"

          IFS=',' read -ra S <<< "${{ github.event.inputs.spiders }}"
          for s in "${S[@]}"; do
            s="$(echo "$s" | xargs)"
            [[ -z "$s" ]] && continue
            echo "::group::scrapy crawl $s"
            scrapy crawl "$s" -s FEEDS="$FEEDS_JSON"
            echo "::endgroup::"
          done

      - name: Upload data (JSONL)
        uses: actions/upload-artifact@v4
        with:
          name: events-jsonl
          # actions/upload-artifact resolves from the repo root, so prefix with scraper/
          path: scraper/${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}
          if-no-files-found: warn
          retention-days: 10

      - name: Upload debug (optional)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-artifacts
          path: |
            scraper/debug/**
            scraper/httpcache/**
          if-no-files-found: warn
          retention-days: 7
