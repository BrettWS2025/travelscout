name: Scrape Events

on:
  workflow_dispatch:
    inputs:
      spiders:
        description: "Comma-separated spider names to run"
        default: "auckland_events"
        required: false
      robots_obey:
        description: "Override ROBOTSTXT_OBEY (true/false). Leave empty to use project setting."
        default: ""
        required: false
      clear_httpcache:
        description: "Clear Scrapy HTTPCACHE before run (true/false)"
        default: "false"
        required: false

jobs:
  run-scrapy:
    name: Run Scrapy spiders
    runs-on: ubuntu-latest
    timeout-minutes: 60

    # Make tscraper importable and wire in settings
    env:
      SCRAPY_SETTINGS_MODULE: tscraper.settings
      PYTHONUNBUFFERED: "1"
      PYTHONIOENCODING: "utf-8"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"
      TS_DEBUG: "1"
      TS_SAVE_HTML: "1"
      # default to auckland_events; user can override in dispatch form
      SPIDERS: ${{ github.event.inputs.spiders || 'auckland_events' }}
      # Add the 'scraper/' folder to module search path so 'tscraper' can be imported
      PYTHONPATH: ${{ github.workspace }}/scraper

    defaults:
      run:
        shell: bash
        # All shell steps execute within the scraper/ directory
        working-directory: scraper

    steps:
      - name: "Checkout repository"
        uses: actions/checkout@v4

      - name: "Set up Python 3.11"
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: "Install Python dependencies"
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: "Cache Playwright browsers"
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-ms-playwright-${{ hashFiles('scraper/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-ms-playwright-

      - name: "Install Playwright (Chromium) with system deps"
        run: |
          python -m playwright install --with-deps chromium

      - name: "Create working folders"
        run: |
          mkdir -p debug data httpcache

      - name: "Preflight: prove tscraper is importable & list spiders"
        run: |
          echo "CWD: $(pwd)"
          echo "PYTHONPATH=$PYTHONPATH"
          echo "SCRAPY_SETTINGS_MODULE=$SCRAPY_SETTINGS_MODULE"
          # Prove Python can import tscraper and show where it loads from
          python -c "import sys; import tscraper; print('tscraper package:', tscraper.__file__)"
          scrapy version -v | tee -a debug/preflight.txt
          scrapy list | tee debug/spiders.txt

      - name: "Optionally clear HTTP cache"
        if: ${{ github.event.inputs.clear_httpcache == 'true' }}
        run: |
          rm -rf httpcache || true
          echo "Cleared httpcache/" | tee -a debug/preflight.txt

      - name: "Run spiders"
        run: |
          set -euo pipefail

          ROBOT_FLAG=""
          if [[ -n "${{ github.event.inputs.robots_obey }}" ]]; then
            v="$(echo "${{ github.event.inputs.robots_obey }}" | tr '[:upper:]' '[:lower:]')"
            if [[ "$v" == "true" || "$v" == "false" ]]; then
              ROBOT_FLAG="-s ROBOTSTXT_OBEY=$v"
            fi
          fi

          echo "Spiders to run: ${SPIDERS}" | tee -a debug/run.meta
          IFS=',' read -ra SARR <<< "${SPIDERS}"

          : > debug/summary.tsv
          echo -e "spider\titems\tstatus" >> debug/summary.tsv

          for sp in "${SARR[@]}"; do
            s="$(echo "$sp" | xargs)"
            [[ -z "$s" ]] && continue
            echo "::group::running ${s}"
            (scrapy crawl "$s" $ROBOT_FLAG 2>&1 | tee "debug/scrapy-${s}.log") || true
            echo "::endgroup::"

            ITEMS=0
            [[ -f data/Events.jsonl ]] && ITEMS=$(wc -l < data/Events.jsonl | tr -d ' ')
            STATUS="ok"
            grep -q "Spider closed (finished)" "debug/scrapy-${s}.log" || STATUS="error"
            echo -e "${s}\t${ITEMS}\t${STATUS}" >> debug/summary.tsv
          done

          echo -e "\n--- SUMMARY ---"
          column -t -s$'\t' debug/summary.tsv || cat debug/summary.tsv

      - name: "Upload scraped data"
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: data-jsonl
          path: scraper/data/*.jsonl
          if-no-files-found: warn
          retention-days: 10

      - name: "Upload debug bundle"
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: debug-artifacts
          path: |
            scraper/debug/**
            scraper/httpcache/**
          if-no-files-found: warn
          retention-days: 10
