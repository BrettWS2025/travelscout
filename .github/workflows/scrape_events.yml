name: Scrape Events (Auckland)

on:
  workflow_dispatch:
    inputs:
      spiders:
        description: "Comma-separated spider names to run"
        required: false
        default: "auckland_events"
      feed_path:
        description: "Relative path (under scraper/) for FEED output"
        required: false
        default: "data/Events.jsonl"
      overwrite:
        description: "Overwrite the output file? (true/false). Use false to append when running multiple spiders."
        required: false
        default: "true"

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    env:
      # Make sure Scrapy loads your project settings module
      SCRAPY_SETTINGS_MODULE: tscraper.settings
      PYTHONUNBUFFERED: "1"
      PYTHONIOENCODING: "utf-8"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"

    defaults:
      run:
        shell: bash
        working-directory: scraper

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-ms-playwright-${{ hashFiles('scraper/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-ms-playwright-

      - name: Install Playwright (Chromium) with system deps
        run: |
          python -m playwright install --with-deps chromium

      - name: Prepare folders
        run: |
          mkdir -p data debug

      - name: Show Scrapy env & discovered spiders
        # This should work now that working-directory is 'scraper' and SCRAPY_SETTINGS_MODULE is set.
        run: |
          echo "SCRAPY_SETTINGS_MODULE=$SCRAPY_SETTINGS_MODULE"
          scrapy version -v
          scrapy list | tee debug/spiders.txt

      - name: Run spiders
        run: |
          set -euo pipefail
          FEED_PATH="${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}"
          SPIDERS="${{ github.event.inputs.spiders || 'auckland_events' }}"
          OVERWRITE="${{ github.event.inputs.overwrite || 'true' }}"

          # choose -O (overwrite) or -o (append)
          if [[ "${OVERWRITE,,}" == "true" ]]; then
            FEED_FLAG="-O"
            # ensure file is reset if it exists
            : > "${FEED_PATH}"
          else
            FEED_FLAG="-o"
          fi

          echo "Spiders: ${SPIDERS}"
          echo "Output:  ${FEED_PATH} (overwrite=${OVERWRITE})"

          IFS=',' read -ra SARR <<< "${SPIDERS}"
          for s in "${SARR[@]}"; do
            s="$(echo "$s" | xargs)"
            [[ -z "$s" ]] && continue
            echo "running ${s}"
            scrapy crawl "${s}" ${FEED_FLAG} "${FEED_PATH}" 2>&1 | tee "debug/scrapy-${s}.log"
          done

      - name: Quick result peek
        run: |
          if [[ -f "${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}" ]]; then
            echo "Events.jsonl lines: $(wc -l < "${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}")"
            head -n 5 "${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}" || true
          else
            echo "No output file found at ${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}."
          fi

      - name: Upload data (jsonl)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: data-jsonl
          path: scraper/data/*.jsonl
          if-no-files-found: warn
          retention-days: 14

      - name: Upload debug logs
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: debug-artifacts
          path: scraper/debug/**
          if-no-files-found: warn
          retention-days: 14
