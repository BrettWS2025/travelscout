name: Scrape events

on:
  workflow_dispatch:
    inputs:
      spiders:
        description: "Comma-separated spider names to run"
        default: "auckland_events"
        required: true
      feed_path:
        description: "Relative path under scraper/ to write the JSON Lines feed"
        default: "data/Events.jsonl"
        required: true
      overwrite:
        description: "Overwrite (true) or append (false) to feed file"
        default: "true"
        required: true

jobs:
  scrape:
    name: Run Scrapy spiders
    runs-on: ubuntu-latest

    defaults:
      run:
        shell: bash
        working-directory: scraper

    env:
      # Make Scrapy find your project settings inside ./scraper
      PYTHONPATH: ${{ github.workspace }}/scraper
      SCRAPY_SETTINGS_MODULE: tscraper.settings
      PYTHONUNBUFFERED: "1"
      PYTHONIOENCODING: "utf-8"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright (Chromium) with system deps
        run: |
          python -m playwright install --with-deps chromium

      - name: Prepare folders
        run: |
          mkdir -p debug httpcache "$(dirname "${{ github.event.inputs.feed_path }}")"

      - name: Sanity check import & spiders
        run: |
          python - <<'PY'
          import os, pkgutil
          print("CWD:", os.getcwd())
          print("PYTHONPATH:", os.environ.get("PYTHONPATH"))
          import tscraper, tscraper.spiders
          print("tscraper OK; spiders:", [m.name for m in pkgutil.iter_modules(tscraper.spiders.__path__)])
          PY
          scrapy version -v
          scrapy list | tee debug/spiders.txt

      - name: Run spiders
        run: |
          set -euo pipefail
          FEED_PATH="${{ github.event.inputs.feed_path }}"
          OVERWRITE="${{ github.event.inputs.overwrite }}"
          # Decide append vs overwrite flag:
          FLAG="-O"   # overwrite
          [[ "${OVERWRITE,,}" == "false" ]] && FLAG="-o"  # append if requested

          IFS=',' read -ra S <<< "${{ github.event.inputs.spiders }}"
          for s in "${S[@]}"; do
            s="$(echo "$s" | xargs)"
            [[ -z "$s" ]] && continue
            echo "::group::scrapy crawl $s"
            # Force JSON Lines explicitly; export encoding utf8
            scrapy crawl "$s" \
              -s FEED_FORMAT=jsonlines \
              -s FEED_EXPORT_ENCODING=utf-8 \
              $FLAG "$FEED_PATH"
            echo "::endgroup::"
          done

          # If nothing was exported, ensure the file exists so the artifact step finds it
          [[ -f "$FEED_PATH" ]] || touch "$FEED_PATH"

      - name: Upload data (JSONL)
        uses: actions/upload-artifact@v4
        with:
          name: events-jsonl
          path: scraper/${{ github.event.inputs.feed_path }}
          if-no-files-found: error   # now guaranteed to exist (touched if empty)
          retention-days: 10

      - name: Upload debug bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-artifacts
          path: |
            scraper/debug/**
            scraper/httpcache/**
          if-no-files-found: warn
          retention-days: 7
