name: Scrape events

on:
  workflow_dispatch:
    inputs:
      spiders:
        description: "Comma-separated spider names to run"
        default: "auckland_events"
        required: true
      feed_path:
        description: "Relative path under scraper/ to write the JSON Lines feed"
        default: "data/Events.jsonl"
        required: true

jobs:
  scrape:
    name: Run Scrapy spiders
    runs-on: ubuntu-latest

    defaults:
      run:
        shell: bash
        working-directory: scraper

    env:
      # Make Scrapy load your project
      SCRAPY_SETTINGS_MODULE: tscraper.settings
      # Ensure 'tscraper' is importable when the CWD is 'scraper'
      PYTHONPATH: ${{ github.workspace }}/scraper
      PYTHONUNBUFFERED: "1"
      PYTHONIOENCODING: "utf-8"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright (Chromium) with system deps
        run: |
          python -m playwright install --with-deps chromium

      - name: Prepare folders (debug/httpcache and feed directory)
        run: |
          set -e
          mkdir -p debug httpcache
          FEED_PATH="${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}"
          mkdir -p "$(dirname "$FEED_PATH")"
          # Ensure the file exists even if no items are scraped
          : > "$FEED_PATH"

      - name: Sanity check Python package resolution
        run: |
          python - <<'PY'
          import os, pkgutil
          print("CWD:", os.getcwd())
          print("PYTHONPATH:", os.environ.get("PYTHONPATH"))
          import tscraper, tscraper.spiders
          print("tscraper path:", list(tscraper.__path__))
          print("discovered spiders:", [m.name for m in pkgutil.iter_modules(tscraper.spiders.__path__)])
          PY

      - name: Show Scrapy version & discovered spiders
        run: |
          scrapy version -v
          scrapy list | tee debug/spiders.txt

      - name: Run spiders (write JSONL via -O, store even if empty)
        run: |
          set -euo pipefail
          FEED_PATH="${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}"
          IFS=',' read -ra S <<< "${{ github.event.inputs.spiders }}"
          for s in "${S[@]}"; do
            s="$(echo "$s" | xargs)"
            [[ -z "$s" ]] && continue
            echo "::group::scrapy crawl $s"
            scrapy crawl "$s" \
              -O "$FEED_PATH" \
              -s FEED_FORMAT=jsonlines \
              -s FEED_EXPORT_ENCODING=utf-8 \
              -s FEED_STORE_EMPTY=True
            echo "::endgroup::"
          done

      - name: Inspect resulting file
        run: |
          FEED_PATH="${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}"
          echo "Expecting: $FEED_PATH"
          ls -la "$(dirname "$FEED_PATH")" || true
          echo "Head:"
          head -n 5 "$FEED_PATH" || true
          echo "Line count:"
          wc -l "$FEED_PATH" || true

      - name: Upload data (JSONL)
        uses: actions/upload-artifact@v4
        with:
          name: events-jsonl
          # upload from the repo root, so prefix with scraper/
          path: scraper/${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}
          if-no-files-found: warn
          retention-days: 10

      - name: Upload debug (optional)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-artifacts
          path: |
            scraper/debug/**
            scraper/httpcache/**
          if-no-files-found: warn
          retention-days: 7
