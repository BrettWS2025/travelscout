name: Scrape (Events)

on:
  workflow_dispatch:
    inputs:
      spiders:
        description: "Comma-separated spider names to run"
        default: "auckland_events"
        required: false
      clear_httpcache:
        description: "Clear Scrapy HTTPCACHE before run (true/false)"
        default: "false"
        required: false
  schedule:
    - cron: "30 3 * * *"   # daily at 03:30 UTC
  push:
    branches: [ main ]
    paths:
      - "scraper/**"
      - ".github/workflows/scrape_events.yml"

concurrency:
  group: scrape-events-${{ github.ref }}
  cancel-in-progress: false

jobs:
  run:
    name: Run Scrapy spiders
    runs-on: ubuntu-latest
    timeout-minutes: 60

    env:
      # default; can be overridden via workflow_dispatch input
      SPIDERS: ${{ github.event.inputs.spiders || 'auckland_events' }}
      # make sure Scrapy can import your project (tscraper/settings.py)
      SCRAPY_SETTINGS_MODULE: "tscraper.settings"
      PYTHONUNBUFFERED: "1"
      PYTHONIOENCODING: "utf-8"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"
      # simple, opt-in debug flags some spiders may read
      TS_DEBUG: "1"
      TS_SAVE_HTML: "1"

    defaults:
      run:
        shell: bash
        working-directory: scraper

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-ms-playwright-${{ hashFiles('scraper/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-ms-playwright-

      - name: Install Playwright (Chromium) with system deps
        run: |
          python -m playwright install --with-deps chromium

      - name: Prepare folders
        run: |
          mkdir -p data debug httpcache
          date -u +"Started at %Y-%m-%d %H:%M:%S UTC" | tee debug/run.meta

      - name: Show discovered spiders (quick sanity check)
        run: |
          echo "SCRAPY_SETTINGS_MODULE=$SCRAPY_SETTINGS_MODULE"
          scrapy version -v
          scrapy list | tee debug/spiders.txt

      - name: Optionally clear HTTP cache
        if: ${{ github.event.inputs.clear_httpcache == 'true' }}
        run: rm -rf httpcache || true

      - name: Run spiders
        run: |
          IFS=',' read -ra SARR <<< "$SPIDERS"
          : > debug/summary.tsv
          echo -e "spider\titems\tstatus" >> debug/summary.tsv

          for sp in "${SARR[@]}"; do
            s="$(echo "$sp" | xargs)"
            [ -z "$s" ] && continue
            echo "::group::scrapy crawl ${s}"
            scrapy crawl "$s" 2>&1 | tee "debug/scrapy-${s}.log" || true
            echo "::endgroup::"

            LINES=0
            [ -f data/Events.jsonl ] && LINES=$(wc -l < data/Events.jsonl || echo 0)
            STATUS="ok"
            grep -q "Spider closed (finished)" "debug/scrapy-${s}.log" || STATUS="error"
            echo -e "${s}\t${LINES}\t${STATUS}" >> debug/summary.tsv
          done

          echo -e "\n--- SUMMARY ---"
          (command -v column >/dev/null 2>&1 && column -t -s$'\t' debug/summary.tsv) || cat debug/summary.tsv

      - name: Print first few lines of Events.jsonl
        run: |
          if [ -f data/Events.jsonl ]; then
            echo "Events.jsonl lines: $(wc -l < data/Events.jsonl)"
            head -n 5 data/Events.jsonl
          else
            echo "No data/Events.jsonl produced."
          fi

      - name: Upload scraped data
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: data-jsonl
          path: scraper/data/*.jsonl
          if-no-files-found: warn
          retention-days: 10

      - name: Upload debug bundle
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: debug-artifacts
          path: |
            scraper/debug/**
            scraper/httpcache/**
            scraper/traces/**
            scraper/screens/**
            scraper/screenshots/**
          if-no-files-found: warn
          retention-days: 10
