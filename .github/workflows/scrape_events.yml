name: Scrape Events

on:
  workflow_dispatch:
    inputs:
      spiders:
        description: "Comma-separated spider names to run (default: auckland_events)"
        required: false
        default: "auckland_events"

concurrency:
  group: scrape-events-${{ github.ref }}
  cancel-in-progress: false

jobs:
  scrape:
    name: Run Scrapy spiders
    runs-on: ubuntu-latest
    timeout-minutes: 60

    env:
      # Ensure Scrapy uses your project settings
      SCRAPY_SETTINGS_MODULE: tscraper.settings
      # Default spider(s). You can override when starting a run from the UI.
      SPIDERS: ${{ github.event.inputs.spiders || 'auckland_events' }}
      PYTHONUNBUFFERED: "1"
      PYTHONIOENCODING: "utf-8"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"

    defaults:
      run:
        shell: bash
        working-directory: scraper

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright (Chromium) + system deps
        run: |
          python -m playwright install --with-deps chromium

      - name: Prepare output folders
        run: |
          mkdir -p data
          mkdir -p debug
          mkdir -p httpcache

      - name: Show discovered spiders (for quick sanity check)
        run: scrapy list | tee debug/spiders.txt

      - name: Run spider(s)
        run: |
          IFS=',' read -ra SARR <<< "$SPIDERS"
          for sp in "${SARR[@]}"; do
            s="$(echo "$sp" | xargs)"
            [[ -z "$s" ]] && continue
            echo "Running spider: $s"
            scrapy crawl "$s"
          done

      - name: Print quick results summary
        run: |
          if [[ -f data/Events.jsonl ]]; then
            echo "Events.jsonl lines: $(wc -l < data/Events.jsonl | tr -d ' ')"
            echo "First 3 lines:"
            head -n 3 data/Events.jsonl || true
          else
            echo "No data/Events.jsonl produced."
          fi

      - name: Upload scraped data
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: data-jsonl
          path: scraper/data/*.jsonl
          if-no-files-found: warn
          retention-days: 10

      - name: Upload debug logs
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: debug-artifacts
          path: scraper/debug/**
          if-no-files-found: warn
          retention-days: 10
