name: Scrape events

on:
  workflow_dispatch:
    inputs:
      spiders:
        description: "Comma-separated spider names to run"
        default: "auckland_events"
        required: true
      feed_path:
        description: "Path under scraper/ to write the JSON Lines feed"
        default: "data/Events.jsonl"
        required: true

permissions:
  contents: write  # needed so we can commit the feed back to the repo

jobs:
  scrape:
    name: Run Scrapy spiders
    runs-on: ubuntu-latest

    # All shell steps run from the scraper/ directory
    defaults:
      run:
        shell: bash
        working-directory: scraper

    env:
      SCRAPY_SETTINGS_MODULE: tscraper.settings
      PYTHONPATH: ${{ github.workspace }}/scraper
      PYTHONUNBUFFERED: "1"
      PYTHONIOENCODING: "utf-8"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          persist-credentials: true
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright (Chromium) with system deps
        run: |
          python -m playwright install --with-deps chromium

      - name: Prepare folders
        run: |
          mkdir -p debug httpcache
          mkdir -p "$(dirname "${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}")"

      - name: Sanity check import path
        run: |
          python - <<'PY'
          import os, pkgutil, tscraper
          print("CWD", os.getcwd())
          print("PYTHONPATH", os.environ.get("PYTHONPATH"))
          print("tscraper path:", list(tscraper.__path__))
          import tscraper.spiders
          print("discovered spiders:", [m.name for m in pkgutil.iter_modules(tscraper.spiders.__path__)])
          PY

      - name: Show Scrapy version & discovered spiders
        run: |
          scrapy version -v
          scrapy list | tee debug/spiders.txt

      - name: Run spiders (write JSON Lines feed)
        id: runspiders
        env:
          FEED_PATH: ${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}
          SPIDERS: ${{ github.event.inputs.spiders }}
        run: |
          set -euo pipefail

          # Build a proper FEEDS mapping in JSON so Scrapy doesn't rely on extension guessing
          FEEDS_JSON="$(python - <<'PY'
          import json, os
          p = os.environ.get("FEED_PATH", "data/Events.jsonl")
          # jsonlines exporter is explicit; "overwrite": False => append across spiders if needed
          print(json.dumps({p: {"format": "jsonlines", "encoding": "utf8", "overwrite": False}}))
          PY
          )"

          echo "FEED will go to: ${FEED_PATH}"
          echo "FEEDS mapping: ${FEEDS_JSON}"

          IFS=',' read -ra S <<< "${SPIDERS}"
          for s in "${S[@]}"; do
            s="$(echo "$s" | xargs)"
            [[ -z "$s" ]] && continue
            echo "::group::scrapy crawl $s"
            scrapy crawl "$s" -s FEEDS="$FEEDS_JSON"
            echo "::endgroup::"
          done

      - name: Verify feed file exists
        env:
          FEED_PATH: ${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}
        run: |
          set -e
          ls -la "$(dirname "$FEED_PATH")" || true
          test -s "$FEED_PATH" || (echo "ERROR: Feed file $FEED_PATH not found or empty." && exit 1)
          echo "First 5 lines:"
          head -n 5 "$FEED_PATH" || true

      - name: Upload data (JSONL)
        uses: actions/upload-artifact@v4
        with:
          name: events-jsonl
          # upload from repo root: prefix with scraper/
          path: scraper/${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}
          if-no-files-found: error
          retention-days: 10

      - name: Commit feed back to repository
        # Only try to commit when we're on a branch (e.g., main) and not in a PR from a fork
        if: ${{ github.ref_type == 'branch' }}
        env:
          FEED_PATH: ${{ github.event.inputs.feed_path || 'data/Events.jsonl' }}
        run: |
          set -e
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # We are in scraper/; git works fine from subdirectories
          git add "$FEED_PATH" || true

          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "chore: update ${FEED_PATH} [skip ci]"
          # push to the same branch the workflow is running on
          git push
