name: "Scrape (Auckland Events) - Debug"

on:
  workflow_dispatch:
    inputs:
      spiders:
        description: "Comma-separated spider names to run"
        default: "auckland_events"
        required: false
      robots_obey:
        description: "Override ROBOTSTXT_OBEY (true/false). Leave empty to use project setting."
        default: ""
        required: false
      clear_httpcache:
        description: "Clear Scrapy HTTPCACHE before run (true/false)"
        default: "false"
        required: false
  schedule:
    - cron: "30 3 * * *" # every day at 03:30 UTC
  push:
    branches: [ main ]
    paths:
      - "scraper/**"
      - ".github/workflows/scrape_whatsonevents.yml"

concurrency:
  group: scrape-${{ github.ref }}
  cancel-in-progress: false

jobs:
  scrape:
    name: Run spiders
    runs-on: ubuntu-latest
    timeout-minutes: 60

    env:
      PYTHONUNBUFFERED: "1"
      PYTHONIOENCODING: "utf-8"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"
      # If the dispatch input is present and non-empty, use it; otherwise default to auckland_events
      SPIDERS: ${{ github.event.inputs.spiders && github.event.inputs.spiders || 'auckland_events' }}
      TS_DEBUG: "1"
      TS_SAVE_HTML: "1"
      SCRAPY_SETTINGS_MODULE: "tscraper.settings"

    defaults:
      run:
        shell: bash
        working-directory: scraper

    steps:
      - name: "Checkout repo"
        uses: actions/checkout@v4

      - name: "Show runner info"
        run: |
          uname -a || true
          python3 --version || true
          node --version || true

      - name: "Set up Python 3.11"
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: "Install Python dependencies"
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: "Cache Playwright browsers"
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/ms-playwright
          key: ${{ runner.os }}-ms-playwright-${{ hashFiles('scraper/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-ms-playwright-

      - name: "Install Playwright (Chromium) with system deps"
        run: |
          python -m playwright install --with-deps chromium

      - name: "Prepare debug directories"
        run: |
          mkdir -p debug data httpcache
          echo "Started at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" | tee debug/run.meta

      - name: "Preflight: dump Scrapy config & discovered spiders"
        run: |
          set -e
          scrapy version -v | tee -a debug/preflight.txt
          echo -e "\n--- settings snapshot ---" | tee -a debug/preflight.txt
          # print a few key settings to verify overrides
          for k in BOT_NAME ROBOTSTXT_OBEY DOWNLOAD_DELAY AUTOTHROTTLE_ENABLED AUTOTHROTTLE_START_DELAY AUTOTHROTTLE_MAX_DELAY HTTPCACHE_ENABLED TWISTED_REACTOR PLAYWRIGHT_BROWSER_TYPE; do
            echo "$k=$(scrapy settings --get $k)" | tee -a debug/preflight.txt
          done
          echo -e "\n--- spider list ---" | tee -a debug/preflight.txt
          scrapy list | tee debug/spiders.txt

      - name: "Preflight: fetch sample pages (raw HTML) for troubleshooting"
        run: |
          UA="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36"
          set -x
          curl -fsSL -A "$UA" -D debug/aucklandnz_events_headers.txt -o debug/aucklandnz_events.html https://www.aucklandnz.com/events-hub/events || true
          curl -fsSL -A "$UA" -D debug/the_nutcracker_headers.txt -o debug/the_nutcracker.html https://www.aucklandnz.com/events-hub/events/the-nutcracker-2025 || true
          curl -fsSL -A "$UA" -D debug/pasifika_headers.txt -o debug/pasifika.html https://www.aucklandnz.com/pasifika || true
          set +x

      - name: "Optionally clear HTTP cache"
        if: ${{ github.event.inputs.clear_httpcache == 'true' }}
        run: |
          rm -rf httpcache || true
          echo "Cleared httpcache/" | tee -a debug/preflight.txt

      - name: "Run spiders (with optional ROBOTSTXT_OBEY override)"
        id: runspiders
        run: |
          set -euo pipefail
          set -x

          # Prepare the override flag if provided
          ROBOT_FLAG=""
          if [[ -n "${{ github.event.inputs.robots_obey }}" ]]; then
            ROBOT_VAL="${{ github.event.inputs.robots_obey }}"
            # Normalize to true/false lower-case
            ROBOT_VAL="$(echo "$ROBOT_VAL" | tr '[:upper:]' '[:lower:]')"
            if [[ "$ROBOT_VAL" == "true" || "$ROBOT_VAL" == "false" ]]; then
              ROBOT_FLAG="-s ROBOTSTXT_OBEY=$ROBOT_VAL"
            fi
          fi

          echo "Spiders to run: ${SPIDERS}" | tee -a debug/run.meta
          IFS=',' read -ra SARR <<< "${SPIDERS}"

          : > debug/summary.tsv
          echo -e "spider\titems\tstatus" >> debug/summary.tsv

          # Start with a clean output file for each job run
          : > data/Events.jsonl

          for sp in "${SARR[@]}"; do
            s="$(echo "$sp" | xargs)"
            [[ -z "$s" ]] && continue
            echo "::group::Running $s"
            (scrapy crawl "$s" $ROBOT_FLAG 2>&1 | tee "debug/scrapy-${s}.log") || true
            echo "::endgroup::"

            ITEMS=0
            if [[ -f data/Events.jsonl ]]; then
              ITEMS=$(wc -l < data/Events.jsonl | tr -d ' ')
            fi
            STATUS="ok"
            if ! grep -q "Spider closed (finished)" "debug/scrapy-${s}.log"; then
              STATUS="error"
            fi
            echo -e "${s}\t${ITEMS}\t${STATUS}" >> debug/summary.tsv
          done

          echo -e "\n--- SUMMARY ---"
          if command -v column >/dev/null 2>&1; then
            column -t -s$'\t' debug/summary.tsv || cat debug/summary.tsv
          else
            cat debug/summary.tsv
          fi

      - name: "Print quick results summary"
        run: |
          if [[ -f data/Events.jsonl ]]; then
            echo "Events.jsonl lines: $(wc -l < data/Events.jsonl | tr -d ' ')"
            echo "First 3 lines:"
            head -n 3 data/Events.jsonl || true
          else
            echo "No data/Events.jsonl found."
          fi

      - name: "Upload scraped data"
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: data-jsonl
          path: |
            scraper/data/*.jsonl
          if-no-files-found: warn
          retention-days: 10

      - name: "Upload debug bundle (logs, preflight HTML, httpcache)"
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: debug-artifacts
          path: |
            scraper/debug/**
            scraper/httpcache/**
          if-no-files-found: warn
          retention-days: 10

      - name: "Upload Playwright traces/screens if produced by spiders"
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: playwright-artifacts
          path: |
            scraper/traces/**
            scraper/screens/**
            scraper/screenshots/**
          if-no-files-found: warn
          retention-days: 7
