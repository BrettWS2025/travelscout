name: Whats On and Events

on:
  workflow_dispatch:
    inputs:
      spiders:
        description: "Comma-separated spider names to run (e.g. christchurch_whats_on,auckland_events). Leave blank for all."
        required: false
        default: ""
  schedule:
    - cron: "17 16 1 * *"   # monthly on the 1st at 16:17 UTC

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r scraper/requirements.txt

      - name: Cache Playwright browsers (optional)
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-ms-playwright-${{ hashFiles('scraper/requirements.txt') }}

      - name: Install Playwright browsers (+ OS deps)
        run: python -m playwright install --with-deps chromium

      - name: Decide spider list
        id: decide
        run: |
          ALL="christchurch_whats_on,auckland_events,christchurch_events,auckland_whats_on"
          INPUT="${{ github.event.inputs.spiders }}"
          if [ -z "$INPUT" ]; then LIST="$ALL"; else LIST="$INPUT"; fi
          echo "list=$LIST" >> $GITHUB_OUTPUT

      - name: Run selected spiders (headed Chromium + NZ UA/headers)
        working-directory: scraper
        env:
          # Optional: add a repo secret HOTC_PROXY like http://user:pass@host:port if the site blocks GH runners
          HOTC_PROXY: ${{ secrets.HOTC_PROXY }}
        run: |
          set -euo pipefail
          mkdir -p data

          # ---- Build a Playwright context (UA, locale, headers; optional proxy) as compact JSON ----
          PW_CTX="$(python - <<'PY'
import json, os
UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"
ctx = {
  "default": {
    "user_agent": UA,
    "locale": "en-NZ",
    "extra_http_headers": {
      "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
      "Accept-Language": "en-NZ,en;q=0.9",
      "Upgrade-Insecure-Requests": "1",
      "Sec-Fetch-Site": "none",
      "Sec-Fetch-Mode": "navigate",
      "Sec-Fetch-User": "?1",
      "Sec-Fetch-Dest": "document"
    }
  }
}
proxy = os.environ.get("HOTC_PROXY", "").strip()
if proxy:
    ctx["default"]["proxy"] = {"server": proxy}
print(json.dumps(ctx, separators=(",", ":")))
PY
          )"

          # Run headed Chromium under Xvfb; reduce obvious automation flags
          PW_LAUNCH='{"headless":false,"args":["--disable-blink-features=AutomationControlled","--no-sandbox","--disable-dev-shm-usage"]}'

          IFS=',' read -ra SPIDERS <<< "${{ steps.decide.outputs.list }}"

          EVENTS=()
          THINGS=()
          for s in "${SPIDERS[@]}"; do
            s_trimmed="$(echo "$s" | xargs)"
            case "$s_trimmed" in
              christchurch_events|auckland_events) EVENTS+=("$s_trimmed");;
              christchurch_whats_on|auckland_whats_on) THINGS+=("$s_trimmed");;
              *) echo "Skipping unknown spider: $s_trimmed" ;;
            esac
          done

          run_spider () {
            local spider="$1"; local mode="$2"; local outfile="$3"

            # Build CLI args safely (avoid quoting issues) using a Bash array
            args=(
              -s "CONCURRENT_REQUESTS=16"
              -s "CONCURRENT_REQUESTS_PER_DOMAIN=16"
              -s "DOWNLOAD_DELAY=0.25"
              -s "AUTOTHROTTLE_ENABLED=1"
              -s "ROBOTSTXT_OBEY=False"
              -s "HTTPCACHE_ENABLED=0"
              -s "HTTPERROR_ALLOWED_CODES=403,429,503"
              -s "PLAYWRIGHT_CONTEXTS=$PW_CTX"
              -s "PLAYWRIGHT_LAUNCH_OPTIONS=$PW_LAUNCH"
              -s "PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT=45000"
            )

            if [ "$mode" = "overwrite" ]; then
              xvfb-run -a python -m scrapy crawl "$spider" "${args[@]}" -O "$outfile"
            else
              xvfb-run -a python -m scrapy crawl "$spider" "${args[@]}" -o "$outfile"
            fi
          }

          # -------- Events.jsonl --------
          if [ ${#EVENTS[@]} -gt 0 ]; then
            FIRST=1
            for s in "${EVENTS[@]}"; do
              if [ $FIRST -eq 1 ]; then
                run_spider "$s" "overwrite" "data/Events.jsonl"
                FIRST=0
              else
                run_spider "$s" "append" "data/Events.jsonl"
              fi
            done
          fi

          # -------- Things_to_do.jsonl --------
          if [ ${#THINGS[@]} -gt 0 ]; then
            FIRST=1
            for s in "${THINGS[@]}"; do
              if [ $FIRST -eq 1 ]; then
                run_spider "$s" "overwrite" "data/Things_to_do.jsonl"
                FIRST=0
              else
                run_spider "$s" "append" "data/Things_to_do.jsonl"
              fi
            done
          fi

      - name: Detect data changes
        id: detect
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          ADDED=0
          if [ -f scraper/data/Events.jsonl ]; then git add scraper/data/Events.jsonl; ADDED=1; fi
          if [ -f scraper/data/Things_to_do.jsonl ]; then git add scraper/data/Things_to_do.jsonl; ADDED=1; fi

          if [ "$ADDED" = "0" ]; then
            echo "changed=false" >> $GITHUB_OUTPUT
          elif git diff --cached --quiet; then
            echo "changed=false" >> $GITHUB_OUTPUT
          else
            echo "changed=true" >> $GITHUB_OUTPUT
          fi

      - name: Commit scraped data
        if: steps.detect.outputs.changed == 'true' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
        run: |
          git commit -m "data: update scraped JSONL ($(date -u +'%Y-%m-%dT%H:%M:%SZ'))"
          git push

      - name: Upload artifact (only when changed)
        if: steps.detect.outputs.changed == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: nz-events-and-things-jsonl
          path: |
            scraper/data/Events.jsonl
            scraper/data/Things_to_do.jsonl

      - name: Upload HOTC debug artifacts (if present)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: hotc-debug
          path: |
            scraper/hotc_blocked.html
            scraper/hotc_blocked.png
            scraper/hotc_listing.png
            scraper/debug/
          if-no-files-found: ignore
