# GitHub Actions workflow to run TravelScout scrapers with extra debugging
# Settings live in: tscraper/settings.py
# Helper utilities (parsers, date/price helpers): tscraper/utils.py
name: Scrape (Auckland Events) â€” Debug

on:
  workflow_dispatch:
    inputs:
      spiders:
        description: "Comma-separated spider names to run"
        default: "auckland_events"
        required: false
      robots_obey:
        description: "Override ROBOTSTXT_OBEY (true/false). Leave empty to use project setting."
        default: ""
        required: false
      clear_httpcache:
        description: "Clear Scrapy HTTPCACHE before run (true/false)"
        default: "false"
        required: false
  schedule:
    # Run every day at 03:30 UTC
    - cron: "30 3 * * *"
  push:
    branches: [ "main" ]
    paths:
      - "scraper/**"
      - ".github/workflows/scrape_whatsonevents.yml"

concurrency:
  group: "scrape-${{ github.ref }}"
  cancel-in-progress: false

jobs:
  scrape:
    name: Run spiders
    runs-on: ubuntu-latest
    timeout-minutes: 60

    # Baseline env; inputs are plumbed in step envs for safety
    env:
      PYTHONUNBUFFERED: "1"
      PYTHONIOENCODING: "utf-8"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"
      # Default spider if no workflow_dispatch input provided
      SPIDERS: "auckland_events"
      # Let spiders opt into saving debug artifacts if they read these flags
      TS_DEBUG: "1"
      TS_SAVE_HTML: "1"
      # Ensure Scrapy project uses the repo settings module
      SCRAPY_SETTINGS_MODULE: "tscraper.settings"

    defaults:
      run:
        shell: bash
        working-directory: scraper

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Show runner info
        run: |
          uname -a || true
          python3 --version || true
          node --version || true

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/ms-playwright
          key: ${{ runner.os }}-ms-playwright-${{ hashFiles('scraper/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-ms-playwright-

      - name: Install Playwright (Chromium) with system deps
        run: |
          python -m playwright install --with-deps chromium

      - name: Prepare debug directories
        run: |
          mkdir -p debug data httpcache
          echo "Started at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" | tee debug/run.meta

      - name: Preflight: dump Scrapy config & discovered spiders
        run: |
          scrapy version -v | tee -a debug/preflight.txt
          {
            echo
            echo "--- settings snapshot ---"
            for k in BOT_NAME ROBOTSTXT_OBEY DOWNLOAD_DELAY AUTOTHROTTLE_ENABLED AUTOTHROTTLE_START_DELAY AUTOTHROTTLE_MAX_DELAY HTTPCACHE_ENABLED TWISTED_REACTOR PLAYWRIGHT_BROWSER_TYPE; do
              printf "%s=%s\n" "$k" "$(scrapy settings --get $k)"
            done
            echo
            echo "--- spider list ---"
            scrapy list
          } | tee -a debug/preflight.txt

      - name: Preflight: fetch sample pages (raw HTML) for troubleshooting
        run: |
          UA="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36"
          set -x
          curl -fsSL -A "$UA" -D debug/aucklandnz_events_headers.txt -o debug/aucklandnz_events.html https://www.aucklandnz.com/events-hub/events || true
          # Example detail URLs for selector debugging
          curl -fsSL -A "$UA" -D debug/the_nutcracker_headers.txt -o debug/the_nutcracker.html https://www.aucklandnz.com/events-hub/events/the-nutcracker-2025 || true
          curl -fsSL -A "$UA" -D debug/pasifika_headers.txt -o debug/pasifika.html https://www.aucklandnz.com/pasifika || true
          set +x

      - name: Optionally clear HTTP cache
        # Limit the condition to workflow_dispatch to avoid undefined inputs on push/schedule
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.clear_httpcache == 'true' }}
        run: |
          rm -rf httpcache || true
          echo "Cleared httpcache/" | tee -a debug/preflight.txt

      - name: Run spiders (with optional ROBOTSTXT_OBEY override)
        id: runspiders
        env:
          # Pass inputs safely (empty on non-dispatch events)
          INPUT_SPIDERS: ${{ github.event.inputs.spiders }}
          INPUT_ROBOTS: ${{ github.event.inputs.robots_obey }}
        run: |
          set -euo pipefail
          set -x

          # Start from default env SPIDERS, override if workflow_dispatch provided one
          if [[ -n "${INPUT_SPIDERS:-}" ]]; then
            export SPIDERS="${INPUT_SPIDERS}"
          fi

          # Prepare the ROBOTSTXT_OBEY override flag if provided
          ROBOT_FLAG=""
          if [[ -n "${INPUT_ROBOTS:-}" ]]; then
            ROBOT_VAL="$(echo "${INPUT_ROBOTS}" | tr '[:upper:]' '[:lower:]')"
            if [[ "${ROBOT_VAL}" == "true" || "${ROBOT_VAL}" == "false" ]]; then
              ROBOT_FLAG="-s ROBOTSTXT_OBEY=${ROBOT_VAL}"
            fi
          fi

          echo "Spiders to run: ${SPIDERS}" | tee -a debug/run.meta
          IFS=',' read -ra SARR <<< "${SPIDERS}"

          # Summary file to accumulate item counts per spider
          : > debug/summary.tsv
          printf "spider\titems\tstatus\n" >> debug/summary.tsv

          for sp in "${SARR[@]}"; do
            s="$(echo "$sp" | xargs)"
            [[ -z "$s" ]] && continue
            echo "::group::Running ${s}"
            # Each spider writes to shared feed path configured in settings (data/Events.jsonl)
            # We also tee logs to debug/scrapy-$s.log
            (scrapy crawl "${s}" ${ROBOT_FLAG} 2>&1 | tee "debug/scrapy-${s}.log") || true
            echo "::endgroup::"

            # Count items appended by the spider (Scrapy feed appends by default if overwrite=False)
            ITEMS=0
            if [[ -f data/Events.jsonl ]]; then
              ITEMS=$(wc -l < data/Events.jsonl | tr -d ' ')
            fi
            STATUS="ok"
            if ! grep -q "Spider closed (finished)" "debug/scrapy-${s}.log"; then
              STATUS="error"
            fi
            printf "%s\t%s\t%s\n" "${s}" "${ITEMS}" "${STATUS}" >> debug/summary.tsv
          done

          echo
          echo "--- SUMMARY ---"
          awk -F'\t' 'BEGIN{OFS="\t"}{print $1,$2,$3}' debug/summary.tsv | column -t -s$'\t' || cat debug/summary.tsv

      - name: Print quick results summary
        run: |
          if [[ -f data/Events.jsonl ]]; then
            echo "Events.jsonl lines: $(wc -l < data/Events.jsonl | tr -d ' ')"
            echo "First 3 lines:"
            head -n 3 data/Events.jsonl || true
          else
            echo "No data/Events.jsonl found."
          fi

      - name: Upload scraped data
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: data-jsonl
          path: |
            scraper/data/*.jsonl
          if-no-files-found: warn
          retention-days: 10

      - name: Upload debug bundle (logs, preflight HTML, httpcache)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: debug-artifacts
          path: |
            scraper/debug/**
            scraper/httpcache/**
          if-no-files-found: warn
          retention-days: 10

      - name: Upload Playwright traces/screens if produced by spiders
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: playwright-artifacts
          path: |
            scraper/traces/**
            scraper/screens/**
            scraper/screenshots/**
          if-no-files-found: warn
          retention-days: 7
